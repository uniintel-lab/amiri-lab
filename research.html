<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>MLOL@RPI ECSE: Rsearch</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Machine Learning and Optimization Lab, MLOL @ RPI">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
	  <script src="./js/w3data.js"></script>
   </head>
   <body>
      <div class="container">
         <div w3-include-html="./src/header.html" id="common_header"></div>
         <hr>
         <div class="row-fluid">
            <div class="span12">
               <section id="statemet">
                  <div class="page-header">
                     <h3>Selected Research Directions</h3>
                  </div>
                  <div><p>The following is a list of active research topics we have right now, and we would love to collaborate on other topics in areas of Natural Language Processing and (Robust and Accountable) Machine Learning.
                     </p></div>
               </section>
               <hr>
			   <section id="research-topics">
                  <ul>
					  <li><h4><a onclick="toggleById('topic-1')">Robust Knowledge Acquisition from Human Language</a></h4>
							<p id="topic-1" style="display: none">Knowledge graphs (KGs) provide both open-world and domain-specific knowledge representations that are integral to many AI systems. However, constructing KGs is usually very costly and requires extensive effort. A widely attempted solution is to learn knowledge acquisition models that automatically induce structured knowledge from unstructured text. However, such models developed through data-driven machine learning are usually fragile to noise in learning resources, and may fall short of providing reliable inference on large, heterogeneous real-world data. We are developping a general meta-learning framework that seeks to systematically improve the robustness of learning and inference for data-driven knowledge acquisition models. We seek to solve several key problems to accomplish the goal: (i) How to identify incorrect training labels and prevents overfitting on noisy labels; (ii) how do detect invalid input instances in inference (e.g., out-of-distribution ones) and allow prediction with abstention; (iii) automated constraint learning that ensures inference with global consistency; (iv) model robustness against noise perturbation; (v) mitigating spurious correlation of models captured in biased training.</p>
					  </li>
					   <li><h4><a onclick="toggleById('topic-6')">Indirect Supervision for Knowledge Acquisition</a></h4>
							<p id="topic-6" style="display: none">Knowledge acquisition tasks (e.g., relation extraction, entity and event extraction and typing, consolidation) face challenges including extreme label spaces, insufficient annotations and out-of-distribution prediction. To this end, we study the method for leveraging indirect supervision signals from auxiliary tasks (e.g., natural language inference, abstractive summarization, etc.) to foster robust and generalizable inference for (open-domain) knowledge acquisition or information extraction. In the same context, study the method for generating semantically rich label representations based on either gloss knowledge or structural knowledge from a well-populated lexical knowledge base, in order to better support learning with limited labels.</p>
					  </li>
					  <li><h4><a onclick="toggleById('topic-eq')">Equivariance Learning in NLP</a></h4>
							<p id="topic-eq" style="display: none">Natural language understanding and generation tasks need to handle equivariance properties in data. For example, the narrative structure of an article can be reorganized, while still presenting the same content. In constrained NLG tasks with structural priors (e.g. data-to-text generation tasks), the structure of the prior can also be modified while presenting semantically equivalent content. However, existing sequential modeling of Transformer LMs cause downstream information extraction and NLG systems to be brittle to content-neutral transformations of input data. We are developping equivariance learning methods that allow Transformer models to give consistent output under any such content-neutral perturbations on input data.</p>
					  </li>
					  <li><h4><a onclick="toggleById('topic-3')">Event-Centric Natural Language Understanding</a></h4>
							<p id="topic-3" style="display: none">Human languages evolve to communicate about events happening in the real world. Therefore, understanding events plays a critical role in natural language understanding (NLU). A key challenge to this mission lies in the fact that events are not just simple, standalone predicates. Rather, they are often described at different granularities, temporally form event processes, and are directed by specific central goals in a context. Our research in this line helps the machine understand events described in natural language. This includes the understanding of how events are connected, form processes or structure complices, and the recognition of typical properties of events (e.g., space, time, salience, essentiality, implicitness, memberships, etc.).</p>
					  </li>
					  <li><h4><a onclick="toggleById('topic-2')">Transferable Representation Learning for Structured Knowledge</a></h4>
							<p id="topic-2" style="display: none">Constructing KGs is usually very costly and requires extensive effort. Representation learning offers a solution to this problem, by using relational embeddings to complete KGs. However, when obtaining such embedding representations, each (general or domain-specific) KG has been captured separately. In particular, we study a novel direction of transferable representation learning for KGs,  which seeks to associate the interrelated knowledge from different isolated KGs in a common embedding scheme, and allows complementary knowledge to easily migrate across different KGs. This project will systematically solve several key technical challenges of leveraging incidental and auxiliary supervision to capture various types of knowledge association. The project will develop technologies to support robust inference in a multi-source knowledge transfer setting with noise-aware meta learning and constrained inference, particularly for knowledge curated for low-resource domains and languages.</p>
					  </li>
					  <li><h4><a onclick="toggleById('topic-4')">Machine Commonsense Reasoning</a></h4>
							<p id="topic-4" style="display: none">Various types of commonsense inference tasks are challenging the SOTA language models. Such tasks may include inferring preconditions of facts, typical properties of entities and events (e.g. time, scales and numerical properties), and typical relations (e.g. ordering and membership of events, topological relations of entities). While annotating data for those aspects of commonsense inference can be costly, we seek to minimally leverage any expensive annotations, but instead develop linguistic pattern mining techniques to find vast cheap (though allowably noisy) supervision data from the Web, and lead that towards a scalable and generalizable solution to improve commonsense inference based on distant supervision.</p>
					  </li>
					  <li><h4><a onclick="toggleById('topic-5')">Knowledge Acquisition from Multi-modal Data</a></h4>
							<p id="topic-5" style="display: none">Multi-modal data such as images, videos and tables may contain rich information that is complementary to that in human language. To synthesize actionable knowledge from semi-structural and multimedia that would help downstream NLU tasks (e.g. QA and fact verification), a system needs to has the ability to summarize the salient information and well connect it with natural language. However, this is accompanied with several key challenges: (i) How to find salient information in data of different structures or displays? (ii) Since different subparts of the structure or media represent different knowledge/facts, how to foster controlled natural language generation precisely describe the highlighted knowledge/fact? (iii) How do we enable effective aggregation of information in the generated summaries (e.g. finding the max, averages, or identifying specific patterns)? (iv) How do we summarize in a way that helps downstream tasks for question answering and fact verification?</p>
					  </li>
					 
					  <li><h4><a onclick="toggleById('topic-7')">AI for the Common Good</a></h4>
							<p id="topic-7" style="display: none">We are always interested in applying our discoveries in NLU and data-driven machine learning in areas of biology, medicine, geology and social sciences.</p>
					  </li>
				  <ul>
               </section>
            </div>
         </div>
      </div>
      <div w3-include-html="./src/footer.html" id="common_footer"></div>
	  
      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
	  <script> w3IncludeHTML(include_callback); function include_callback() {$("#nav-research").addClass("active");}</script>
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
		 
		 function toggleById(id) {
			  var x = document.getElementById(id);
			  if (x.style.display === "none") {
				x.style.display = "block";
			  } else {
				x.style.display = "none";
			  }
			}
         
      </script>
   </body>
</html>
