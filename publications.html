
<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>MLOL@RPI: Publications</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Machine Learning and Optimization Lab, MLOL @ RPI">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min_publications.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
<!--       <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" rel="stylesheet"> -->
<!--       <link rel="stylesheet" href="/css/main.css"> -->
	  <style type="text/css">
         <
         .STYLE2 {font-family: Calibri; font-size: 24px; }
         .STYLE27 {font-family: Calibri; font-size: 16px; }
         >
      </style>
	  <script src="./js/w3data.js"></script>
	  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
	  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
</head>
	
   <body>
<div class="container">
         <div w3-include-html="./src/header.html" id="common_header"></div>
         <hr>
         <div class="row-fluid">
            <div class="span12">
               <section id="published">	    
</div></div>  
</li>
</ul>

	

<div class="container-fluid">	
<div class="row">
        <div id="gridid" class="col-sm-12">
  	<h1 id="group-projects-highlights">Group Highlights</h1>

	<div class="row">
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization</b>
	      <p><img src="./img/chen_et_al_2020.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>This paper introduces Stochastic Compositional Optimization, a generalization of classic stochastic optimization to minimize compositions of functions with nested expectations. The proposed Stochastically Corrected Stochastic Compositional gradient (SCSC) method guarantees convergence at the same rate as SGD for non-compositional stochastic optimization and achieves state-of-the-art performance, particularly when combined with the Adam optimization technique. SCSC is applied and tested in various tasks, including model-agnostic meta-learning and risk-averse portfolio management.</p>
	      <p><em><span style="color:blue">T. Chen</span>, <span style="color:blue">Y. Sun</span>, <span style="color:blue">W. Yin</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2008.10847">ICASSP’21</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>CAFE: Catastrophic Data Leakage in Vertical Federated Learning</b>
	      <p><img src="./img/jin_et_al_2021.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>The paper highlights the risk of private data leakage in distributed machine learning systems like federated learning, through the gradients sharing mechanism. It challenges the defense strategy of increasing batch size to prevent data recovery and introduces a new data leakage attack called "catastrophic data leakage in vertical federated learning" (CAFE). The proposed attack efficiently recovers batch data from shared aggregated gradients. The study demonstrates CAFE's effectiveness in performing large-batch data leakage attacks with improved data recovery quality. The paper also suggests a practical countermeasure to mitigate CAFE and emphasizes the high risk of private data leakage in standard federated learning settings, particularly in vertical cases.</p>
	      <p><em><span style="color:blue">X. Jin</span>, <span style="color:blue">P. Chen</span>, <span style="color:blue">C. Hsu</span>, <span style="color:blue">C. Yu</span>, <span style="color:blue">T. Chen</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2110.15122">NeurIPS'21</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	</div>

	<div class="row">
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>A Single-Timescale Method for Stochastic Bilevel Optimization</b>
	      <p><img src="./img/chen_et_al_2022.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>The paper introduces a novel optimization method called SingleTimescale stochAstic BiLevEl optimization (STABLE) for stochastic bilevel optimization, where the objective depends on the solution of another optimization problem. STABLE uses a single-loop fashion with a fixed batch size, achieving similar sample complexity as stochastic gradient descent (SGD) for single-level optimization. For e-stationary point, it requires O(e^(-2)) samples, and for e-optimal solution in strongly convex cases, O(e^(-1)) samples. STABLE is the first bilevel optimization algorithm with comparable sample complexity to SGD.</p>
	      <p><em><span style="color:blue">T. Chen</span>, <span style="color:blue">Y. Sun</span>, <span style="color:blue">, <span style="color:blue">Q. Xiao, </span>W. Yin</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2102.04671">AISTATS'22</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning</b>
	      <p><img src="./img/Abbas_et_al_2022.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>This paper studies Model-Agnostic Meta Learning (MAML) through the lens of Sharpness-Aware Minimization. We propose Sharp-MAML, a new approach developed to optimize MAML effectively, tackling its complex loss landscape with numerous saddle points and local minimizers. By leveraging sharpness-aware minimization, Sharp-MAML outperforms the plain-vanilla MAML baseline, achieving a notable increase in accuracy on Mini-Imagenet (e.g., +3%). The paper provides empirical evidence, convergence rate analysis, and generalization bounds for Sharp-MAML, making it the first study of its kind in the context of bilevel learning.</p>
	      <p><em><span style="color:blue">M. Abbas</span>, <span style="color:blue">Q. Xiao</span>, <span style="color:blue">L. Chen</span>, <span style="color:blue">T. Chen</span>, <span style="color:blue">P. Chen</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2206.03996">ICML'22</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	</div>

	<div class="row">
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>On Penalty-based Bilevel Gradient Descent Method </b>
	      <p><img src="./img/shen_et_al_2023.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>This paper explores bilevel optimization and its applications in hyper-parameter optimization, meta-learning, and reinforcement learning. It addresses the difficulty of solving bilevel problems and introduces the penalty method as a solution. The proposed penalty-based bilevel gradient descent (PBGD) algorithm demonstrates finite-time convergence for constrained bilevel problems without lower-level strong convexity. Experimental results showcase the efficiency of PBGD, highlighting its potential for scalable bilevel optimization in various scenarios.</p>
	      <p><em><span style="color:blue">H. Shen</span>, <span style="color:blue">Q. Xiao</span>, <span style="color:blue">T. Chen</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2302.05185">ICML’23</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	
	  <div class="col-sm-6" style="width: 46%">
	    <div class="well">
	      <b>Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance</b>
	      <p><img src="./img/chen_et_al_2023.png" class="img-responsive" width="100%" style="float: left" /></p>
	      <p>This study focuses on Multi-objective learning (MOL) problems, where multiple learning criteria or tasks are present. Dynamic weighting algorithms like MGDA aim to find update directions that avoid conflicts among objectives. However, empirical findings show that dynamic weighting methods might not consistently outperform static ones. This work delves into a stochastic variant of MGDA called MoDo, exploring its generalization performance and the interplay with optimization. Surprisingly, it reveals that the key rationale behind MGDA could hinder dynamic weighting algorithms from achieving the optimal O(1/√n) population risk, leading to unique trade-offs in MOL.</p>
	      <p><em><span style="color:blue">L.Chen</span>, <span style="color:blue">H. Fernando</span>, <span style="color:blue">Y. Ying</span>, <span style="color:blue">T. Chen</span>, <span style="color:blue">P. Chen</span></em></p>
	      <p><strong><a href="https://arxiv.org/abs/2305.20057">Under Submission</a></strong></p>
	      <p class="text-danger"><strong> </strong></p>
	      <p> </p>
	    </div>
	  </div>
	</div>
</div>
</div>
</div>

	
		      
</ul>
<h2>Representative/Recent Works</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2302.05185.pdf">On Penalty-based Bilevel Gradient Descent Method</a></p>
<ul>
<li><p>H. Shen, Q. Xiao and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of International Conference on Machine Learning</i> <b>(ICML)</b>, Honolulu, HI, July 22-29, 2023.</p>
</li>  
</ul>     
<li><p><a href="https://arxiv.org/pdf/2106.13781.pdf">Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems</a></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun and W. Yin</p>
  </li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Virtual, December 6-14, 2021. <b>(Spotlight)</b></p>
</li>  
  </ul>
</li>  
<li><p><a href="https://arxiv.org/pdf/2203.03059.pdf" >Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?</a></p>
<ul>
<li><p>L. Chen and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Virtual, March 28 - 30, 2022.</p>
</li>   
  </ul>  
</li>    
<li><p><a href="https://arxiv.org/pdf/2008.10847.pdf">Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization</a></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun and W. Yin</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 69, pp. 4937-4948, June 2021.</p>  
</li>
 <li><p>The conference version has received the <b>2021 ICASSP Best Student Paper Award</b>.</p>    
  </li>  
  </ul>
</li>      
<li><p><a href="https://arxiv.org/pdf/2110.15122.pdf" >Catastrophic Data Leakage in Vertical Federated Learning</a></p>
<ul>
<li><p>X. Jin, P.-Y. Chen, C.-Y. Hsu, C.-M. Yu, and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Virtual, December 6-14, 2021.</p>
</li>  
</ul>   
<li><p><a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a> <a href="https://www.videoken.com/embed/-pywxDhaBzc?tocitem=8"><b>[Talk]</b></a>, <a href="https://www.dropbox.com/s/m80ytd2kqn4b72p/poster_nips.pdf?dl=0"><b>[Poster]</b></a>, and <a href="https://github.com/chentianyi1991/LAG-code"><b>[Code]</b></a></p>
<ul>
<li><p><b>T. Chen</b>, G. B. Giannakis, T. Sun and W. Yin</p>
</li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Montreal, Canada, December 3-8, 2018. <b>(Spotlight)</b></p>
</li>
</ul>
</li>   
<!-- <li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8442.pdf">An Online Convex Optimization Approach to Proactive Network Resource Allocation</a></p>
<ul>
<li><p><b>T. Chen</b>, Q. Ling and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 24, pp. 6350-6364, December 2017.</p>
  </li></ul>
</li>  -->
</ul>

 
  
<h2>Dissertation</h2>
<ul>
<li> <a href="https://conservancy.umn.edu/bitstream/handle/11299/206678/Chen_umn_0130E_20409.pdf?sequence=1&isAllowed=y">Efficient Methods for Distributed Machine Learning and Resource Management in IoT</a></p>
<ul><li><p>Ph. D. Dissertation, University of Minnesota, Twin Cities, June 2019.</p>  
</li>
<li><p><b>2020 IEEE Signal Processing Society Best PhD Dissertation Award</b>.</p>    
  </li>  
</ul>
</li>
</ul>

<h2>Journal papers</h2>
<ol reversed>
<li><p><a href="https://arxiv.org/pdf/2012.15511.pdf">Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup</a></p>
<ul>
<li><p>H. Shen, K. Zhang, M. Hong and <b>T. Chen</b></p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 71, to appear, July 2023.</p>    
  </li></ul>
</li>     
<li><p><a href="https://arxiv.org/pdf/2002.08537.pdf">Adaptive Temporal Difference Learning with Linear Function Approximation</a></p>
<ul>
<li><p>T. Sun, H. Shen, <b>T. Chen</b> and D. Li</p>
</li>
<li><p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, to appear, 2021.</p>
  </li></ul>
</li>     
<li><p><a href="https://arxiv.org/pdf/2002.11360.pdf">LASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning</a></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun and W. Yin</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 69, pp. 4637 - 4651, July 2021.</p>    
  </li></ul>
</li>   
<li><p><a href="https://arxiv.org/pdf/2008.10847.pdf">Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization</a></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun and W. Yin</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 69, pp. 4937 - 4948, June 2021.</p>  
</li></ul>
</li>    
<li><p><a href="https://arxiv.org/pdf/2009.11146.pdf">Byzantine-Resilient Decentralized TD Learning with Linear Function Approximation.</a></p>
<ul>
<li><p>Z. Wu, H. Shen, <b>T. Chen</b>, and Q. Ling</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 69, pp. 3839 - 3853, June 2021.</p>
</li></ul>
</li>     
<li><p><a href="https://arxiv.org/pdf/1812.03239.pdf">Communication-Efficient Policy Gradient Methods for Distributed Reinforcement Learning</a></p>
<ul>
<li><p><b>T. Chen</b>, K. Zhang, G. B. Giannakis, and T. Ba&#351;ar</p>
</li>
<li><p><i>IEEE Transactions on Control of Network Systems</i>, to appear, 2021.</p>
  </li></ul>
</li>    
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8582.pdf">Lazily Aggregated Quantized Gradient Innovation for Communication-Efficient Federated Learning</a></p>
<ul>
<li><p>J. Sun, <b>T. Chen</b> and G. B. Giannakis, Z. Yang</p>
</li>
<li><p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, to appear, 2021.</p>
  </li></ul>
</li>    
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9153949">Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks</a></p>
<ul>
<li><p>Z. Wu, Q. Ling, <b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 68, pp. 4583-4596, December 2020.</p>
  </li></ul>
</li>    
<li><p><a href="https://ieeexplore.ieee.org/document/8882321">Secure Mobile Edge Computing in IoT via Collaborative Online Learning</a></p>
<ul>
<li><p>B. Li, <b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 67, no. 23, pp. 5922-5935, December 2019.</p>
  </li></ul>
</li>  
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8516.pdf">Learning and Management for Internet-of-Things: Accounting for Adaptivity and Scalability</a></p>
<ul>
<li><p><b>T. Chen</b>, S. Barbarossa, X. Wang, G. B. Giannakis and Z.-L. Zhang</p>
</li>
<li><p><i> Proceedings of the IEEE</i>, vol. 107, no. 4, pp. 778-796, April 2019.</p>   
</li></ul>
</li>
<li><p><a href="http://www.jmlr.org/papers/volume20/18-030/18-030.pdf">Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics</a></p>
<ul>
<li><p>Y. Shen, <b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i> Journal of Machine Learning Research</i>, vol. 20, no. 22, pp. 1-36, February 2019.</p> 
</li></ul>
</li>
<li><p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214260">Real-time Optimal Energy Management with Reduced Battery Capacity Requirements</a></p>
<ul>
<li><p>B. Li, <b>T. Chen</b>, X. Wang and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Smart Grid</i>, vol. 10, no. 2, pp. 1928-1938, March 2019.</p>
  </li></ul>
</li>
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8470.pdf">Bandit Convex Optimization for Scalable and Dynamic IoT Management</a></p>
<ul>
<li><p><b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Internet of Things Journal</i>, vol. 6, no. 1, pp. 1276-1286, February 2019.</p>
</li></ul>
</li> 
<!-- <li><p><a href="https://arxiv.org/pdf/1804.07051.pdf">Multi-Timescale Online Optimization of Network Function Virtualization for Service Chaining</a></p>
<ul>
<li><p>X. Chen, W. Ni, <b>T. Chen</b>, I. Collins, X. Wang and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Mobile Computing</i>, vol. 18, no. 12, pp. 2899-2912, December 2019.</p>
</li></ul>
</li>     -->
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8498.pdf">Heterogeneous Online Learning for "Thing-Adaptive'' Fog Computing in IoT</a></p>
<ul>
<li><p><b>T. Chen</b>, Q. Ling, Y. Shen and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Internet of Things Journal</i>, vol. 5 , no. 6 , pp. 4328 - 4341, December 2018.</p>
</li></ul>
</li>     
<li><p><a href="https://arxiv.org/abs/1703.01673">Learn-and-Adapt Stochastic Dual Gradients for Network Resource Allocation</a></p>
<ul>
<li><p><b>T. Chen</b>, Q. Ling and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Control of Network Systems</i>, vol. 5, no. 4, pp. 1941-1951, December 2018.</p>
  </li></ul>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/7549052/">Two-Scale Stochastic Control for Multipoint Communication Systems with Renewables</a></p>
<ul>
<li><p>X. Wang, X. Chen, <b>T. Chen</b>, L. Huang and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Smart Grid</i>, vol. 9, no. 3, pp. 1822 - 1834, May. 2018.</p>
  </li></ul>
</li>
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8442.pdf">An Online Convex Optimization Approach to Proactive Network Resource Allocation</a></p>
<ul>
<li><p><b>T. Chen</b>, Q. Ling and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 24, pp. 6350-6364, Dec. 2017.</p>
  </li></ul>
</li>
<li><p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214260">Real-time Energy Trading and Future Planning for Fifth-Generation Wireless Communications</a></p>
<ul>
<li><p>X. Chen, W. Ni, <b>T. Chen</b>, I. Collins, X. Wang and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Wireless Communications Magazine</i>, vol.24, no. 4, pp. 24-30, Aug. 2017.</p>
  </li></ul>
</li>
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8422.pdf">Stochastic Averaging for Constrained Optimization with Application to Online Resource Allocation</a></p>
<ul>
<li><p><b>T. Chen</b>, A. Mokhtari, X. Wang, A. Ribeiro and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 12, pp. 3078-3093, Jun. 2017.</p>
  </li></ul>
</li>
<!-- <li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8400.pdf">DGLB: Distributed Stochastic Geographical Load Balancing over cloud networks</a></p>
<ul>
<li><p><b>T. Chen</b>, A. Marques and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Transactions on Parallel and Distributed Systems</i>, vol. 28, no. 7, pp. 1866-1880, July 2017.</p>
   </li></ul>
</li> -->
<li><p><a href="https://ieeexplore.ieee.org/document/7544445/">Dynamic Resource Allocation for Smart-Grid Powered MIMO Downlink Transmissions</a></p>
<ul>
<li><p>X. Wang, <b>T. Chen</b>, X. Chen, X. Zhou and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Journal on Selected Areas in Communications</i>, Vol. 34, No. 12, pp. 3354 - 3365, Dec. 2016.</p>
   </li></ul>
</li>
<!-- <li><p><a href="https://ieeexplore.ieee.org/document/7389362/">Dynamic Energy Management for Smart-Grid-Powered Coordinated Multipoint Systems</a></p>
<ul>
<li><p>X. Wang, Y. Zhang, <b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Journal on Selected Areas in Communications</i>, vol. 65, no. 12, pp. 3078-3093, Jun. 2016.</p>
   </li></ul>
</li> -->
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8373.pdf">Robust Workload and Energy Management for Sustainable Data Centers</a></p>
<ul>
<li><p><b>T. Chen</b>, Y. Zhang X. Wang, and G. B. Giannakis</p>
</li>
<li><p><i>IEEE  Journal on Selected Areas in Communications</i>, Vol. 34, No. 3, pp. 651-664, Mar. 2016.</p>
   </li></ul>
</li>
<li><p><a href="http://www.dtc.umn.edu/s/resources/spincom8353.pdf">Cooling-Aware Energy and Workload Management in Data Centers via Stochastic Optimization</a></p>
<ul>
<li><p><b>T. Chen</b>, X. Wang and G. B. Giannakis</p>
</li>
<li><p><i>IEEE Journal on Special Topics in Signal Processing</i>, Vol. 10, No. 2, pp. 402-415, Mar. 2016.</p>
  </li></ul>
</li>
<!-- <li><p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219479">Energy-Efficient Transmission Schedule for Delay-Limited Bursty Data Arrivals under Non-Ideal Circuit Power Consumption</a></p>
<ul>
<li><p> Z. Nan, <b>T. Chen</b>, X. Wang and W. Ni</p>
</li>
<li><p><i>IEEE Transaction on Vehicular Technology</i>, Vol. 65, No. 8, pp. 6588 - 6600, Aug. 2016.</p>
  </li></ul>
</li> -->
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/6922592/">Optimal Scheduling for Wireless On-Demand Data Packet Delivery to High-Speed Trains</a></p>
<ul>
<li><p><b>T. Chen</b>, H. Shan and X. Wang</p>
</li>
<li><p><i>IEEE Transaction on Vehicular Technology</i>, Vol. 64, No. 9, pp. 4101 - 4112, Sept. 2015.</p>
  </li></ul>
</li>
<li><p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001259">Optimal MIMO Broadcasting for Energy Harvesting Transmitter with Non-ideal Circuit Power Consumption</a></p>
<ul>
<li><p>X. Wang, Z. Nan and <b>T. Chen</b></p>
</li>
<li><p><i>IEEE Transaction on Wireless Communication</i>, Vol. 14, No. 5, pp. 2500 - 2512, May 2015.</p>
</li></ul>
</ol>
  
</li>
</ul>
<h2>Selected Conference papers (not updated)</h2> 
<ol reversed>
<li><p><a href="https://arxiv.org/pdf/2302.05185.pdf">On Penalty-based Bilevel Gradient Descent Method</a></p>
<ul>
<li><p>H. Shen, Q. Xiao and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of International Conference on Machine Learning</i> <b>(ICML)</b>, Honolulu, HI, July 22-29, 2023.</p>
</li>  
</ul>   
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/10097096">A Nested Ensemble Method to Bilevel Machine Learning</a></p>
<ul>
<li><p>L. Chen, M. Abbas, and <b>T. Chen</b></p>
  </li>
<li><p><i>Proc. of Intl. Conf. on Acoustics, Speech, and Signal Processing</i> <b>(ICASSP)</b>, Rhodes Island, Greece, June 4-10, 2023.</p>
</li>  
</ul>     
<li><p><a href="https://arxiv.org/pdf/2210.12624.pdf" >Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach</a> <b>(Oral)</b></p>
<ul>
<li><p>H. Fernando, H. Shen, M Liu, S Chaudhury, K Murugesan and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Intl. Conf. on Learning Representations</i> <b>(ICLR)</b>, Kigali, Rwanda, May 1 - 5, 2023.</p>
</li>  
</ul>    
<li><p><a href="https://arxiv.org/pdf/2211.07096.pdf" >Alternating Implicit Projected SGD and Its Efficient Variants for Equality-constrained Bilevel Optimization</a></p>
<ul>
<li><p>Q. Xiao, H. Shen, W. Yin and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Valencia, Spain, April 25 - 27, 2023.</p>
</li>  
</ul>  
<li><p><a href="https://proceedings.mlr.press/v206/shen23b/shen23b.pdf" >Distributed Offline Policy Optimization Over Batch Data</a></p>
<ul>
<li><p>H. Shen, S. Lu, X. Cui and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Valencia, Spain, April 25 - 27, 2023.</p>
</li>  
</ul>    
<li><p><a href="https://arxiv.org/pdf/2206.13482.pdf">Understanding Benign Overfitting in Gradient-based Meta Learning</a></p>
<ul>
<li><p>L. Chen, S. Lu, and <b>T. Chen</b></p>
  </li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, New Orleans, LA, November 28-December 9, 2022.</p>
</li>  
</ul>    
<li><p><a href="https://arxiv.org/pdf/2206.10414.pdf">A Single-Timescale Analysis For Stochastic Approximation With Multiple Coupled Sequences</a> <b>(Oral)</b></p>
<ul>
<li><p>H. Shen and <b>T. Chen</b></p>
  </li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, New Orleans, LA, November 28-December 9, 2022.</p>
</li>  
</ul>
<li><p><a href="https://proceedings.mlr.press/v162/abbas22b/abbas22b.pdf">Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning</a></p>
<ul>
<li><p>M. Abbas*, Q. Xiao*, L. Chen*, P.-Y. Chen, and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of International Conference on Machine Learning</i> <b>(ICML)</b>, Baltimore, MD, July 17-23, 2022.</p>
</li>  
</ul>      
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9747833">Federated Multi-armed Bandit via Uncoordinated Exploration</a></p>
<ul>
<li><p>Z. Yan, Q. Xiao, <b>T. Chen</b> and A. Tajer</p>
  </li>
<li><p><i>Proc. of Intl. Conf. on Acoustics, Speech, and Signal Processing</i> <b>(ICASSP)</b>, Virtual, May 22-27, 2022.</p>
</li>  
</ul>   
<li><p><a href="https://arxiv.org/pdf/2203.03059.pdf" >Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?</a></p>
<ul>
<li><p>L. Chen and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Virtual, March 28 - 30, 2022.</p>
</li>  
</ul>  
<li><p><a href="https://arxiv.org/pdf/2102.04671" >A Single-Timescale Method for Stochastic Bilevel Optimization</a> <b>(Oral)</b></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun, Q. Xiao and W. Yin</p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Virtual, March 28 - 30, 2022.</p>
</li>  
</ul>    
<li><p><a href="https://arxiv.org/pdf/2106.13781.pdf">Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems</a>  <b>(Spotlight)</b></p>
<ul>
<li><p><b>T. Chen</b>, Y. Sun and W. Yin</p>
  </li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Virtual, December 6-14, 2021.</p>
</li>  
</ul>
<li><p><a href="https://arxiv.org/pdf/2110.15122.pdf" >Catastrophic Data Leakage in Vertical Federated Learning</a></p>
<ul>
<li><p>X. Jin, P.-Y. Chen, C.-Y. Hsu, C.-M. Yu, and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Virtual, December 6-14, 2021.</p>
</li>  
</ul> 
<li><p><a href="https://arxiv.org/pdf/2106.13781.pdf">A Stochastic Compositional Optimization Method with Applications to Meta
Learning</a> <b>(Best Student Paper)</b></p>
<ul>
<li><p>Y. Sun, <b>T. Chen</b>, and W. Yin</p>
  </li>
<li><p><i>Proc. of Intl. Conf. on Acoustics, Speech, and Signal Processing</i> <b>(ICASSP)</b>, Virtual, June 6-11, 2021.</p>
</li>  
</ul> 
<li><p><a href="http://proceedings.mlr.press/v130/chen21a/chen21a.pdf" >CADA: Communication-Adaptive Distributed Adam</a></p>
<ul>
<li><p><b>T. Chen</b>, Z. Guo, Y. Sun, and W. Yin</p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Virtual, April 16-18, 2021.</p>
</li>  
</ul>    
<li><p><a href="https://chentianyi1991.github.io/aaai.pdf" >Decentralized policy gradient descent ascent for safe
multi-agent reinforcement learning</a></p>
<ul>
<li><p>S. Lu, K. Zhang, <b>T. Chen</b>, T. Başar, and L. Horesh,</p>
</li>
<li><p><i>Proc. of the Assoc. for the Advanc. of Artificial Intelligence </i> <b>(AAAI)</b>, Virtual, February 2-9, 2021.</p>
</li>  
</ul>   
<li><p><a href="https://arxiv.org/pdf/2012.12420.pdf" >Hybrid Federated Learning: Algorithms and Implementation</a> <b>(Best Student Paper)</b></p>
<ul>
<li>X. Zhang, W. Yin, M. Hong, and <b>T. Chen</b></p>
</li>
<li><p><i>Proc. of NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</i>, Virtual, December 12, 2020.</p>
</li></ul> 
<li><p><a href="https://arxiv.org/pdf/2007.06081.pdf" >VAFL: a Method of Vertical Asynchronous Federated Learning</a></p>
<ul>
<li><p><b>T. Chen</b>, X. Jin, Y. Sun and W. Yin</p>
</li>
<li><p><i>Proc. of ICML Workshop on Federated Learning for User Privacy and Data Confidentiality</i>, Virtual, July 17-18, 2020.</p>
</li></ul>   
<li><p><a href="https://arxiv.org/pdf/1909.07588.pdf" >Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients</a></p>
<ul>
<li><p>J. Sun*, <b>T. Chen</b>*, G. B. Giannakis, and Z. Yang <b>(*equal contribution)</b></p>
</li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Vancouver, Canada, December 8-14, 2019.</p>
</li></ul>   
<li><p><a href="http://proceedings.mlr.press/v89/li19d/li19d.pdf">Bandit Online Learning with Unknown Delays</a></p>
<ul>
<li><p>B. Li, <b>T. Chen</b>, and G. B. Giannakis</p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Naha, Japan,  April 16-18, 2019.</p>
</li></ul>  
<li><p><a href="https://arxiv.org/pdf/1811.03761.pdf">RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets</a></p>
<ul>
<li><p>L. Li, W. Xu, <b>T. Chen</b>, G. B. Giannakis, and Q. Ling</p>
</li>  
<li><p><i>Proc. of the Assoc. for the Advanc. of Artificial Intelligence </i> <b>(AAAI)</b>, Honolulu, Hawai, January 27-February 1, 2019.</p>
</li></ul>   
<li><p><a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a></p>
<ul>
<li><p><b>T. Chen</b>, G. B. Giannakis, T. Sun and W. Yin</p>
</li>
<li><p><i>Proc. of Neural Information Processing Systems</i> <b>(NeurIPS)</b>, Montreal, Canada, December 3-8, 2018.</p>
</li>
<li><p><a href="https://www.videoken.com/embed/-pywxDhaBzc?tocitem=8"><b>Spotlight talk</b></a>, <a href="https://www.dropbox.com/s/m80ytd2kqn4b72p/poster_nips.pdf?dl=0"><b>Poster</b></a>, and <a href="https://github.com/chentianyi1991/LAG-code"><b>Matlab code</b></a>.  </p>
</li>  
</ul>
</li>    
<li><p><a href="http://proceedings.mlr.press/v84/shen18a/shen18a.pdf">Online Ensemble Multi-kernel Learning Adaptive to Non-stationary and Adversarial Environments</a></p>
<ul>
<li><p>Y. Shen*, <b>T. Chen</b>* and G. B. Giannakis</p>
</li>
<li><p><i>Proc. of Intl. Conf. on Artificial Intelligence and Statistics</i> <b>(AISTATS)</b>, Lanzarote, Canary Islands, April 9-11, 2018.</p>
</li></ul>
<!-- <li><p>Aggregating Flexibility of Heterogeneous Energy Resources in Distribution Networks</p>
<ul>
<li><p><b>T. Chen</b>, N. Li and G. B. Giannakis</p>
</li>
<li><p><i>Proc. of American Control Conference</i> <b>(ACC)</b>, Milwaukee, WI, June 27-29, 2018.</p>
</li></ul>     -->
<!-- <li><p>Harnessing Bandit Online Learning for Low-Latency Fog Computing in IoT</p>
<ul> -->
<!-- <li><p><b>T. Chen</b> and G. B. Giannakis</p>
</li>
<li><p><i>Proc. of Intl. Conf. on Acoustics, Speech, and Signal Processing</i> <b>(ICASSP)</b>, Calgary, Canada, April 15-20, 2018.</p>
</li></ul>   -->
<li><p>Online Learning for `Thing-Adaptive' Fog Computing in IoT <b>(Best Student Paper Finalist)</b></p>
<ul>
<li><p><b>T. Chen</b>, Y. Shen, Q. Ling and G. B. Giannakis</p>
</li>
<li><p><i>Proc. of Asilomar Conference</i>, Pacific Grove, CA, Oct. 29 - Nov. 1, 2017.</p>
</li></ul>    
</ol> 
                        
               </section>
               <hr>
            </div>
         </div>
      </div>
      <div w3-include-html="./src/footer.html" id="common_footer"></div>

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
	  <script> w3IncludeHTML(include_callback); function include_callback() {$("#nav-publication").addClass("active");}</script>
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
         
      </script>
   </body>
</html>
